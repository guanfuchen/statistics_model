# 支持向量机
支持向量机是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机包括核技巧，可以转换为非线性分类器。支持向量机的学习策略是间隔最大化，包括软间隔、硬间隔等等，从而转换为一个求解凸二次规划的问题，等价于正则化的合叶损失的最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。
支持向量机学习方法包含构建由简至繁的模型：线性可分支持向量机、线性不可分支持向量机、非线性支持向量机。简单模型是复杂模型的基础，也是复杂模型的特殊情况。
- 当训练数据线性可分时，即线性可分支持向量机，又称为硬间隔支持向量机；
- 当训练数据近似可分时，通过软间隔最大化，学习到一个线性的分类器为线性支持向量机，也称为软间隔支持向量机；
- 当训练数据线性不可分时，通过使用核技巧和软间隔最大化，学习非线性支持向量机。

## 线性支持向量机

### 线性可分支持向量机

假设输入空间与特征空间为两个不同的空间。输入空间为欧式空间或离散集合，特征空间为欧式空间或希尔伯特空间。线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的输入映射到特征空间中的特征向量。非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量。所以，输入由输入空间转换为特征空间，支持向量机的学习是在特征空间进行的。
假设给定一个特征空间上的训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$，其中$x_i \in X = R^n，y_i \in Y = {+1, -1}，i=1,2,...,N$，$x_i$为第$i$个特征向量，也称为实例，$y_i$为$x_i$的类标记，当$y_i=+1$时，称$x_i$为正例，当$y_i=-1$时，称$x_i$为负例，$(x_i,y_i)$称为样本点。
学习的目标是在特征空间中找到一个分离超平面，将实例分到不同的类。分离超平面对应于方程$wx+b=0$，由法向量$w$和截距$b$决定，可用(w,b)来表示。分离超平面将特征空间划分为正类和负类。
一般，当训练数据是线性可分时，存在无穷个分离超平面将两类数据分开。感知机利用误差分类最小的策略，求得分离超平面，这时的解有无穷多个，而线性可分支持向量机利用间隔最大化求最优分离超平面，使得解是唯一的。

定义（线性可分支持向量机）：给定线性可分训练数据集，通过间隔最大化或等价地求解得到的分离超平面为：
$$\star{w}x+\star{b}=0$$
以及相应的分类决策函数：
$$f(x)=sign(\star{w}x+\star{b})$$
称为线性可分支持向量机，如下图所示为一条将分割正负类别的分类线。

![](http://chenguanfuqq.gitee.io/tuquan/img_2018_3/bin_classify.png)

讲解支持向量机中的间隔最大化概念之前，首先介绍函数间隔和几何间隔。

#### 函数间隔和几何间隔

上图中，A，B，C三个点表示3个实例，均在分离超平面的正类一侧，预测它们的类。点A距离超平面较远，若预测为正类比较正确，而C距离分离超平面较近，若预测为正类就不那么确信，B在A与C之间，预测准确度也在A和C之间。使用样本点距离分离超平面的距离来度量预测准确性，同时预测在分离超平面的位置（上方还是下方）来表示分类的正确性，综上可使用$y(wx+b)$来表示分类的正确性及确信度，也就是函数间隔。

定义（函数间隔）对于给定的训练集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为
$$\hat{\gamma_i}=y_i(wx_i+b)=$$
定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔之最小值，即
$$\hat{\gamma}=\min_{i=1,...,N}{\hat{\gamma_i}}$$
如果等比例改变$w$和$b$，函数间隔也等比例改变，但是超平面并没有改变，也就是函数间隔无法选择唯一的超平面，这里将分离超平面的法向量$w$增加一个约束，如规范化，$||w||=1$，那么这个确定的间隔被称为几何间隔。
定义（几何间隔）对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为：
$$\gamma_{i}=y_i(\frac{w}{||w||}x_i+\frac{b}{||w||})$$
同样定义超平面的结合间隔为所有样本点的几何间隔最小值，即
$$\gamma=\min_{i=1,...,N}{\gamma_i}$$

#### 间隔最大化
间隔最大化（硬间隔最大化）：选择正确分类线性可分的训练数据集中的硬间隔最大的一个分离超平面，这个分离超平面存在且唯一。那么将该问题建模为如下的约束最优化问题：
$$\max_{w,b} \gamma$$
$$满足约束 y_i(\frac{w}{||w||}x_i+\frac{b}{||w||}) \geq \gamma，i=1,2,...,N$$
如下图所示，其中间隔边界为$wx+b=1$和$wx+b=-1$，这两个间隔平面的中间平面为分离超平面$wx+b=0$，其中间隔最大表示为$\frac{2}{||w||}$。
![](http://chenguanfuqq.gitee.io/tuquan/img_2018_3/support_svm_example.png)
通过上述将问题转换为如下最优化问题，最大化$\frac{2}{||w||}$，相当于最小化$\frac{1}{2}{||w||}^2$：
$$\min_{w,b} \frac{1}{2}{||w||}^2$$
$$满足约束 y_i(wx_i+b)-1 \geq 0, i=1,2,...,N$$
从而转换为求解凸二次规划问题，这里参考拉格朗日乘子法和KKT条件，用来求解约束条件下的最优化问题。

算法（线性可分支持向量机学习算法-最大间隔法）
输入：线性可分训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$
输出：最大间隔分离超平面和分类决策函数
- 构造并求解约束最优化问题：
$$\min_{w,b} \frac{1}{2}{||w||}^2$$
$$满足约束 y_i(wx_i+b)-1 \geq 0, i=1,2,...,N$$
求得最优解$w^{*}$，$b^{*}$
- 由此得到分离超平面和分类决策函数：
$$w^{*}x+b^{*}=0$$
$$f(x)=sign(w^{*}x+b^{*})$$

**最大间隔分离超平面的存在唯一性**，证明略（参考统计学习P101），简述：若训练数据集$T$线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。

#### 支持向量和间隔边界
在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量，即满足该约束的样本点：$y_i(w x_i+b)-1=0$，其中正例所在的支持向量的超平面为$w x_i+b=1$，记为$H_1$，同理负例所在的支持向量超平面为$w x_i+b=-1$，记为$H_2$。$H_1$和$H_2$之间的距离称为间隔，这两个超平面称为间隔边界。由图可知，在决定分离超平面时只有支持向量起作用，其他实例点并不起作用，由于支持向量在确定分离超平面中起着决定性作用，所以将这种模型称为支持向量机。

#### 学习的对偶算法
将原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法。使用该算法的优点：对偶问题更容易求解，自然引入核函数，可以推广到非线性分类问题。
**具体的推导过程如下所述**
- 第一步
$$\min_{w,b} \frac{1}{2}{||w||}^2$$
$$满足约束 y_i(wx_i+b)-1 \geq 0, i=1,2,...,N$$
- 第二步
为了使用拉格朗日乘子法，将不等式约束均转换为小于
$$\min_{w,b} \frac{1}{2}{||w||}^2$$
$$满足约束 1-y_i(wx_i+b) \leq 0, i=1,2,...,N$$
- 第三步
引入拉格朗日乘子$\alpha_i \geq 0, i=1,2,...,N$，那么对应的拉格朗日函数为：
$$L(w,b,\alpha)=\frac{1}{2}{||w||}^2+\sum_{i=1}^{N}{\alpha_i(1-y_i(wx_i+b))}$$
其中，$\alpha=(\alpha_1,\alpha_2,...,\alpha_N)^T$为拉格朗日乘子向量
- 第四步
使用拉格朗日对偶性，得到原始问题的对偶问题$\max_{\alpha} \min_{w,b} L(w,b,\alpha)$
- 第五步
求解$\min_{w,b} L(w,b,\alpha)$，分别对$w$和$b$求偏导数并令其为0，得到：
$$\nabla_{w}L(w,b,\alpha)=w-\sum_{i=1}^{N}{\alpha_i y_i x_i}=0$$
$$\nabla_{b}L(w,b,\alpha)=-\sum_{i=1}^{N}{\alpha_i y_i}=0$$
化简为：
$$w=\sum_{i=1}^{N}{\alpha_i y_i x_i}$$
$$\sum_{i=1}^{N}{\alpha_i y_i}=0$$
带入拉格朗日函数，得到：
$$L(w,b,\alpha)=-\frac{1}{2} w^T w+\sum_{i=1}^{N}{\alpha_i}=-\frac{1}{2} \sum_{i=1}^{N}{\sum_{j=1}^{N}{\alpha_i \alpha_j y_i y_j (x_i x_j)}}+\sum_{i=1}^{N}{\alpha_i}$$
- 第六步
求$\min_{w,b} L(w,b,\alpha)$对$\alpha$的极大
$$\max_{\alpha} -\frac{1}{2} \sum_{i=1}^{N}{\sum_{j=1}^{N}{\alpha_i \alpha_j y_i y_j (x_i x_j)}}+\sum_{i=1}^{N}{\alpha_i}$$
$$满足约束 \sum_{i=1}^{N}{\alpha_i y_i}=0$$
$$\alpha_i \geq 0, i=1,2,...,N$$
转坏为求极小值
$$\min_{\alpha} \frac{1}{2} \sum_{i=1}^{N}{\sum_{j=1}^{N}{\alpha_i \alpha_j y_i y_j (x_i x_j)}}-\sum_{i=1}^{N}{\alpha_i}$$
$$满足约束 \sum_{i=1}^{N}{\alpha_i y_i}=0$$
$$\alpha_i \geq 0, i=1,2,...,N$$
- 第七步
求得$\alpha^{*}=(\alpha_1^{*},\alpha_2^{*},...,\alpha_N^{*})^T$是上述最优化解，那么存在下标$j$，使得$\alpha_{j}^{*} > 0$，带入得到$w^{*}$和$b^{*}$
$$w^{*}=\sum_{i=1}{N}{\alpha_i^{*} y_i x_i}$$
$$b^{*}=y_j - \sum_{i=1}{N}{\alpha_i^{*} y_i (x_i x_j)}$$


### 线性不可分支持向量机

## 非线性支持向量机

## 核函数
当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于在高维的特征空间中学习线性支持向量机。这种方法被称为核技巧。


## SMO算法

SMO是用于快速求解SVM的
它选择凸二次规划的两个变量，其他的变量保持不变，然后根据这两个变量构建一个二次规划问题，这个二次规划关于这两个变量解会更加的接近原始二次规划的解，通过这样的子问题划分可以大大增加整个算法的计算速度，关于这两个变量：

- 其中一个是严重违反KKT条件的一个变量
- 另一个变量是根据自由约束确定，好像是求剩余变量的最大化来确定的。

[支持向量机（五）SMO算法](http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html)

## 测试

## 实现











































