# SVD

奇异值分解（singular value decomposition）是线性代数中一种重要的矩阵分解，在信号处理、统计学等领域中都有重要应用。对称矩阵特征向量分解的基础是谱分析，而奇异值分解则是谱分析理论在任意矩阵上的推广（不限制矩阵的维度）。

## 定义
假设$M$是$nxm$阶矩阵，其中元素全部属于域$K$，也就是实数域或者复数域。那么存在下述分解被称为SVD分解。

$$M=U \Sigma V^{H}$$

其中$U$是$mxm$阶的酉矩阵，$\Sigma$是$mxn$阶的非负实数对角矩阵，$V^{*}$是$V$的共轭转置，$V$是$nxn$阶酉矩阵。这样的分解被称为$M$的奇异值分解。$\Sigma$对角线上的元素为$M$的奇异值。常见的做法是将奇异值由大到小排列，那么$\Sigma$唯一确定。

求解$U$、$V$和$\Sigma$，
$$M^H M = (U \Sigma V^{H})^{H} U \Sigma V^{H}= V \Sigma^{H} U^{H} U \Sigma V^{H}= V \Sigma^{H} \Sigma V^{H} = V (\Sigma^{H} \Sigma) V^{H} \tag{1}$$

$$M M^H = U (\Sigma \Sigma^{H}) U^{H} \tag{2}$$

公式（1）可以通过求解$M^H M$的特征分解来获取$V$的值，公式（2）可以求解$U$的值，同时特征值为$\sigma_{i}^2$，那么通过特征值可以求解$\Sigma$。

## 与特征值分解的联系

奇异值能够用于任意$mxn$矩阵，而特征分解只能使用与特定类型的方阵，故奇异值分解的适用范围更广。其中两者之间的关联可以通过公式（1）、（2）体现出来，求取奇异值分解的$U$、$V$和$\Sigma$需要通过$M^H M$和$M M^H$的特征值分解。

## 应用

### 求广义逆（伪逆）
奇异值分解可以被用来计算矩阵的广义逆（伪逆）。若矩阵$M$的奇异值分解为$M=U \Sigma V^{H}$，那么$M$的伪逆为：
$$M^{+}=V \Sigma^{+} U^{H}$$
其中$\Sigma^{+}$是$\Sigma$的伪逆，是将$\Sigma$主对角线上每个非零元素都求倒数之后再转置得到的。求伪逆通常可以用来求解最小二乘问题。

### 矩阵近似值

奇异值分解在统计中的主要应用是主成分分析（PCA）。数据集的特征值按照重要性排列，降维的过程就是舍弃不重要的特征向量的过程，而剩下的特征向量长成空间为降维后的空间。

---
## 参考资料

[奇异值分解](https://zh.wikipedia.org/wiki/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3) 奇异值分解维基百科。

MIT 线性代数 [第30集] 奇异值分解

[奇异值分解(SVD)原理与在降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html) 刘建平大神SVD讲解。

[谈谈矩阵的 SVD 分解](https://liam0205.me/2017/11/22/SVD-for-Human-Beings/) 非常好的SVD以及其他矩阵知识总结，同时提供了SVD在图像压缩上的应用。

[机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)
