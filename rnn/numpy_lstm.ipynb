{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('../data/rnn/shakespear.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data and calculate indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 99993 characters, 62 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions and Derivatives\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n",
    "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}\n",
    "\n",
    "#### Tanh\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random weights with normal distribution (`0`, `weight_sd`) for $tanh$ activation function and (`0.5`, `weight_sd`) for $sigmoid$ activation function.\n",
    "\n",
    "Biases are initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.W_f = Param('W_f', \n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_f = Param('b_f',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_i = Param('W_i',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_i = Param('b_i',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_C = Param('W_C',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd)\n",
    "        self.b_C = Param('b_C',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_o = Param('W_o',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_o = Param('b_o',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = Param('W_v',\n",
    "                         np.random.randn(X_size, H_size) * weight_sd)\n",
    "        self.b_v = Param('b_v',\n",
    "                         np.zeros((X_size, 1)))\n",
    "        \n",
    "    def all(self):\n",
    "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
    "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
    "        \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "![LSTM](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
    "\n",
    "*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*\n",
    "\n",
    "#### Concatenation of $h_{t-1}$ and $x_t$\n",
    "\\begin{align}\n",
    "z & = [h_{t-1}, x_t] \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### LSTM functions\n",
    "\\begin{align}\n",
    "f_t & = \\sigma(W_f \\cdot z + b_f) \\\\\n",
    "i_t & = \\sigma(W_i \\cdot z + b_i) \\\\\n",
    "\\bar{C}_t & = tanh(W_C \\cdot z + b_C) \\\\\n",
    "C_t & = f_t * C_{t-1} + i_t * \\bar{C}_t \\\\\n",
    "o_t & = \\sigma(W_o \\cdot z + b_t) \\\\\n",
    "h_t &= o_t * tanh(C_t) \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Logits\n",
    "\\begin{align}\n",
    "v_t &= W_v \\cdot h_t + b_v \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Softmax\n",
    "\\begin{align}\n",
    "\\hat{y_t} &= \\text{softmax}(v_t)\n",
    "\\end{align}\n",
    "\n",
    "$\\hat{y_t}$ is `y` in code and $y_t$ is `targets`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev, p = parameters):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "    \n",
    "    z = np.row_stack((h_prev, x))\n",
    "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
    "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
    "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
    "\n",
    "    C = f * C_prev + i * C_bar\n",
    "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, v, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "#### Loss\n",
    "\n",
    "\\begin{align}\n",
    "L_k &= -\\sum_{t=k}^T\\sum_j y_{t,j} log \\hat{y_{t,j}} \\\\\n",
    "L &= L_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Gradients\n",
    "\n",
    "\\begin{align}\n",
    "dv_t &= \\hat{y_t} - y_t \\\\\n",
    "dh_t &= dh'_t + W_y^T \\cdot dv_t \\\\\n",
    "do_t &= dh_t * \\text{tanh}(C_t) \\\\\n",
    "dC_t &= dC'_t + dh_t * o_t * (1 - \\text{tanh}^2(C_t))\\\\\n",
    "d\\bar{C}_t &= dC_t * i_t \\\\\n",
    "di_t &= dC_t * \\bar{C}_t \\\\\n",
    "df_t &= dC_t * C_{t-1} \\\\\n",
    "\\\\\n",
    "df'_t &= f_t * (1 - f_t) * df_t \\\\\n",
    "di'_t &= i_t * (1 - i_t) * di_t \\\\\n",
    "d\\bar{C}'_{t-1} &= (1 - \\bar{C}_t^2) * d\\bar{C}_t \\\\\n",
    "do'_t &= o_t * (1 - o_t) * do_t \\\\\n",
    "dz_t &= W_f^T \\cdot df'_t \\\\\n",
    "     &+ W_i^T \\cdot di_t \\\\\n",
    "     &+ W_C^T \\cdot d\\bar{C}_t \\\\\n",
    "     &+ W_o^T \\cdot do_t \\\\\n",
    "\\\\\n",
    "[dh'_{t-1}, dx_t] &= dz_t \\\\\n",
    "dC'_t &= f_t * dC_t\n",
    "\\end{align}\n",
    "\n",
    "* $dC'_t = \\frac{\\partial L_{t+1}}{\\partial C_t}$ and $dh'_t = \\frac{\\partial L_{t+1}}{\\partial h_t}$\n",
    "* $dC_t = \\frac{\\partial L}{\\partial C_t} = \\frac{\\partial L_t}{\\partial C_t}$ and $dh_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_{t}}{\\partial h_t}$\n",
    "* All other derivatives are of $L$\n",
    "* `target` is target character index $y_t$\n",
    "* `dh_next` is $dh'_{t}$ (size H x 1)\n",
    "* `dC_next` is $dC'_{t}$ (size H x 1)\n",
    "* `C_prev` is $C_{t-1}$ (size H x 1)\n",
    "* $df'_t$, $di'_t$, $d\\bar{C}'_t$, and $do'_t$ are *also* assigned to `df`, `di`, `dC_bar`, and `do` in the **code**.\n",
    "* *Returns* $dh_t$ and $dC_t$\n",
    "\n",
    "#### Model parameter gradients\n",
    "\n",
    "\\begin{align}\n",
    "dW_v &= dv_t \\cdot h_t^T \\\\\n",
    "db_v &= dv_t \\\\\n",
    "\\\\\n",
    "dW_f &= df'_t \\cdot z^T \\\\\n",
    "db_f &= df'_t \\\\\n",
    "\\\\\n",
    "dW_i &= di'_t \\cdot z^T \\\\\n",
    "db_i &= di'_t \\\\\n",
    "\\\\\n",
    "dW_C &= d\\bar{C}'_t \\cdot z^T \\\\\n",
    "db_C &= d\\bar{C}'_t \\\\\n",
    "\\\\\n",
    "dW_o &= do'_t \\cdot z^T \\\\\n",
    "db_o &= do'_t \\\\\n",
    "\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, C_bar, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "        \n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    p.W_o.d += np.dot(do, z.T)\n",
    "    p.b_o.d += do\n",
    "\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "    p.W_C.d += np.dot(dC_bar, z.T)\n",
    "    p.b_C.d += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    p.W_i.d += np.dot(di, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    p.W_f.d += np.dot(df, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    dz = (np.dot(p.W_f.v.T, df)\n",
    "         + np.dot(p.W_i.v.T, di)\n",
    "         + np.dot(p.W_C.v.T, dC_bar)\n",
    "         + np.dot(p.W_o.v.T, do))\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear gradients before each backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip gradients to mitigate exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -1, 1, out=p.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
    "\n",
    "* `input`, `target` are list of integers, with character indexes.\n",
    "* `h_prev` is the array of initial `h` at $h_{-1}$ (size H x 1)\n",
    "* `C_prev` is the array of initial `C` at $C_{-1}$ (size H x 1)\n",
    "* *Returns* loss, final $h_T$ and $C_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    global paramters\n",
    "    \n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
    "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
    "    v_s, y_s =  {}, {}\n",
    "    \n",
    "    # Values at t - 1\n",
    "    h_s[-1] = np.copy(h_prev)\n",
    "    C_s[-1] = np.copy(C_prev)\n",
    "    \n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = np.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "        \n",
    "        (z_s[t], f_s[t], i_s[t],\n",
    "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
    "        v_s[t], y_s[t]) = \\\n",
    "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "            \n",
    "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
    "        \n",
    "    clear_gradients()\n",
    "\n",
    "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = \\\n",
    "            backward(target = targets[t], dh_next = dh_next,\n",
    "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
    "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
    "                     y = y_s[t])\n",
    "\n",
    "    clip_gradients()\n",
    "        \n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = np.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "    \n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
    "        x = np.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Adagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the graph and display a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "    \n",
    "    # Get predictions for 200 letters with current model\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
    "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.m += p.d * p.d # Calculate sum of gradients\n",
    "        #print(learning_rate * dparam)\n",
    "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delay the keyboard interrupt to prevent the training \n",
    "from stopping in the middle of an iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class DelayedKeyboardInterrupt(object):\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "        print('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, pointer = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAFkCAYAAABijEI3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJEmSJEmSJEmSJEmSJEmS1Gv+f3k+i+8p1ZI0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108627ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ered wechoul sharf rhas nound the mit, ubit. turear, bea, tand:\n",
      "WId ole co re\n",
      "Ablar maro;\n",
      "Mthut pirEeR:\n",
      "Whit llard\n",
      "TiGlLNespneonIs nat thiml lorn diil ofas rerled\n",
      "\n",
      "lly thor irer mxon tut of se ans:\n",
      "Wh \n",
      "----\n",
      "iter 18500, loss 58.826335\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            # Reset\n",
    "            if pointer + T_steps >= len(data) or iteration == 0:\n",
    "                g_h_prev = np.zeros((H_size, 1))\n",
    "                g_C_prev = np.zeros((H_size, 1))\n",
    "                pointer = 0\n",
    "\n",
    "\n",
    "            inputs = ([char_to_idx[ch] \n",
    "                       for ch in data[pointer: pointer + T_steps]])\n",
    "            targets = ([char_to_idx[ch] \n",
    "                        for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "\n",
    "            loss, g_h_prev, g_C_prev = \\\n",
    "                forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # Print every hundred steps\n",
    "            if iteration % 100 == 0:\n",
    "                update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "            update_paramters()\n",
    "\n",
    "            plot_iter = np.append(plot_iter, [iteration])\n",
    "            plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "            pointer += T_steps\n",
    "            iteration += 1\n",
    "    except KeyboardInterrupt:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Check\n",
    "\n",
    "Approximate the numerical gradients by changing parameters and running the model. Check if the approximated gradients are equal to the computed analytical gradients (by backpropagation).\n",
    "\n",
    "Try this on `num_checks` individual paramters picked randomly for each weight matrix and bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate numerical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_numerical_gradient(param, idx, delta, inputs, target, h_prev, C_prev):\n",
    "    old_val = param.v.flat[idx]\n",
    "    \n",
    "    # evaluate loss at [x + delta] and [x - delta]\n",
    "    param.v.flat[idx] = old_val + delta\n",
    "    loss_plus_delta, _, _ = forward_backward(inputs, targets,\n",
    "                                             h_prev, C_prev)\n",
    "    param.v.flat[idx] = old_val - delta\n",
    "    loss_mins_delta, _, _ = forward_backward(inputs, targets, \n",
    "                                             h_prev, C_prev)\n",
    "    \n",
    "    param.v.flat[idx] = old_val #reset\n",
    "\n",
    "    grad_numerical = (loss_plus_delta - loss_mins_delta) / (2 * delta)\n",
    "    # Clip numerical error because analytical gradient is clipped\n",
    "    [grad_numerical] = np.clip([grad_numerical], -1, 1) \n",
    "    \n",
    "    return grad_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient of each paramter matrix/vector at `num_checks` individual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_check(num_checks, delta, inputs, target, h_prev, C_prev):\n",
    "    global parameters\n",
    "    \n",
    "    # To calculate computed gradients\n",
    "    _, _, _ =  forward_backward(inputs, targets, h_prev, C_prev)\n",
    "    \n",
    "    \n",
    "    for param in parameters.all():\n",
    "        #Make a copy because this will get modified\n",
    "        d_copy = np.copy(param.d)\n",
    "\n",
    "        # Test num_checks times\n",
    "        for i in range(num_checks):\n",
    "            # Pick a random index\n",
    "            rnd_idx = int(uniform(0, param.v.size))\n",
    "            \n",
    "            grad_numerical = calc_numerical_gradient(param,\n",
    "                                                     rnd_idx,\n",
    "                                                     delta,\n",
    "                                                     inputs,\n",
    "                                                     target,\n",
    "                                                     h_prev, C_prev)\n",
    "            grad_analytical = d_copy.flat[rnd_idx]\n",
    "\n",
    "            err_sum = abs(grad_numerical + grad_analytical) + 1e-09\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / err_sum\n",
    "            \n",
    "            # If relative error is greater than 1e-06\n",
    "            if rel_error > 1e-06:\n",
    "                print('%s (%e, %e) => %e'\n",
    "                      % (param.name, grad_numerical, grad_analytical, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_i (1.065814e-09, 7.992574e-10) => 9.303665e-02\n",
      "W_o (-2.525088e-04, -2.525066e-04) => 4.369793e-06\n",
      "W_o (-4.138911e-07, -4.154312e-07) => 1.854712e-03\n",
      "W_o (3.552714e-09, 5.126857e-09) => 1.626253e-01\n",
      "W_o (5.593392e-06, 5.593947e-06) => 4.955974e-05\n",
      "W_o (2.310969e-05, 2.310855e-05) => 2.480891e-05\n",
      "b_f (-2.593481e-08, -2.802176e-08) => 3.797446e-02\n",
      "b_i (0.000000e+00, 8.953391e-13) => 8.945382e-04\n",
      "b_o (3.440618e-04, 3.440605e-04) => 1.894987e-06\n",
      "b_o (-1.319123e-06, -1.316649e-06) => 9.380262e-04\n",
      "b_o (-7.105427e-10, -8.206268e-10) => 4.349137e-02\n",
      "b_o (1.101341e-08, 1.721758e-08) => 2.122464e-01\n",
      "b_o (-8.856610e-04, -8.856576e-04) => 1.922996e-06\n",
      "b_o (1.716067e-05, 1.715713e-05) => 1.032539e-04\n",
      "b_o (1.691092e-07, 1.693143e-07) => 6.043815e-04\n"
     ]
    }
   ],
   "source": [
    "gradient_check(10, 1e-5, inputs, targets, g_h_prev, g_C_prev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
