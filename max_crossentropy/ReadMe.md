# 最大熵模型

最大熵maximum entropy model由最大熵原理推导实现

## 最大熵原理
最大熵原理是概率模型学习的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为**在满足约束条件下的模型集合中选取熵最大的模型。**
假设离散随机变量X的概率分布是$P(X)$，熵定义为$H(P)=-\sum_{x} P(x)\log P(x)$，其中熵满足不等式$0<=H(P)<=log|X|$，式中，$|X|$是$X$的取值个数，最大值成立当且仅当$X$是均匀分布。也就是说，当$X$服从均匀分布时，**熵最大**，最大熵原理就是利用这个性质来通过熵的最大化来表示等可能性，使得“等可能”这个不容易操作的概念，转换为对熵这个数值指标进行优化。

## 最大熵模型的定义
最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型。假设分类模型是一个条件概率分布$P(Y|X)$，$X$表示输入，$Y$表示输出，这个模型表示了对于给定输入$X$，以条件概率$P(Y|X)$输出$Y$。给定一个训练数据集$T={(x_1,y_1), (x_2,y_2), ..., (x_N,y_N)}$，学习的目标使用最大熵原理选择最好的分类模型。通过训练集确定联合分布$P(X,Y)$和边缘分布$P(X)$的经验分布，记为
$$\hat{P} (X=x, Y=y)=\frac{v(X=x,Y=y)}{N}$$
$$\hat{P} (X=x)=\frac{v(X=x)}{N}$$
其中，$v(X=x)$表示训练数据中样本$(x,y)$出现的频数，$v(X=x)$表示训练数据中输入$x$出现的频数，N表示训练样本的容量。
用特征函数feature function $f(x,y)$来描述输入x和输出y之间的某一个Event，其定义：

$$f(x, y)=
\begin{cases}
1& \text{x与y满足某一事实Event}\\
0& \text{否则}
\end{cases}$$

$f(x,y)$是一个二值函数，当$x$和$y$满足这个事实时取值为1，否则取值为0。
记$E_{\hat(p)}(f)$为特征函数关于经验分布$\hat{P}(X,Y)$的期望值：

$$E_{\hat{P}}(f)=\sum_{x,y}\hat{P}(x,y)f(x,y)$$

记$E_{p}(f)$为特征函数关于经验分布$\hat{P}(X)$和模型$P(Y|X)$的期望值：

$$E_{p}(f)=\sum_{x,y}\hat{P}(x)P(y|x)f(x,y)$$

基于模型能够获取训练数据中的信息，那么以下这两个期望值可以假设为相等，即：

$$E_{\hat{p}}(f)=E_{p}(f)$$

根据以上说明确定最大熵模型的定义：假设满足所有约束条件的模型集合为

![](http://chenguanfuqq.gitee.io/tuquan/img_2018_3/max_crossentropy.png)

## 最大熵模型的学习

最大熵模型的学习过程就是求解最大熵模型的过程。该学习可以形式化为约束最优化问题。

![](http://chenguanfuqq.gitee.io/tuquan/img_2018_3/max_crossentropy_learning.png)

## 模型学习的最优化算法

逻辑斯蒂回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解。似然函数是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解。常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法，其中牛顿法一般收敛速度更快。

### 改进的迭代尺度法

### 拟牛顿法

[最大熵模型学习](https://heshenghuan.github.io/2016/02/26/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0/)
[最大熵模型原理小结](http://www.cnblogs.com/pinard/p/6093948.html)

## 测试

## 实现

















