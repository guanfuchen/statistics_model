{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\u4f7f\u7528 ONNX \u5c06\u6a21\u578b\u4ece PyTorch \u8fc1\u79fb\u5230 Caffe2 \u548c Mobile\n================================================================\n\n\u5728\u672c\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 ONNX \u5c06 PyTorch \u4e2d\u5b9a\u4e49\u7684\u6a21\u578b\u8f6c\u6362\u4e3a ONNX \u683c\u5f0f, \u7136\u540e\u5c06\u5176\u52a0\u8f7d\u5230 Caffe2 \u4e2d.\n\u4e00\u65e6\u8fdb\u5165 Caffe2 , \u6211\u4eec\u53ef\u4ee5\u8fd0\u884c\u8be5\u6a21\u578b\u4ee5\u4ed4\u7ec6\u68c0\u67e5\u5b83\u662f\u5426\u6b63\u786e\u5bfc\u51fa, \u7136\u540e\u6f14\u793a\u5982\u4f55\u4f7f\u7528 Caffe2 \u529f\u80fd\uff08\u4f8b\u5982\u79fb\u52a8\u5bfc\u51fa\u5668\uff09\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u6a21\u578b.\n\n\n\u5728\u672c\u6559\u7a0b\u4e2d, \u60a8\u9700\u8981\u5b89\u88c5 `onnx <https://github.com/onnx/onnx>`__,\n`onnx-caffe2 <https://github.com/onnx/onnx-caffe2>`__ \u548c `Caffe2 <https://caffe2.ai/>`__.\n\u4f60\u53ef\u4ee5\u901a\u8fc7 ``conda install -c ezyang onnx onnx-caffe2`` \u7528 onnx \u548c onnx-caffe2 \u83b7\u5f97\u4e8c\u8fdb\u5236\u7248\u672c.\n\n``NOTE``: \u672c\u6559\u7a0b\u9700\u8981 PyTorch \u4e3b\u5206\u652f, \u53ef\u4ee5\u6309\u7167 `here <https://github.com/pytorch/pytorch#from-source>`__ \u7684\u8bf4\u660e\u8fdb\u884c\u5b89\u88c5\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some standard imports\nimport io\nimport numpy as np\n\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Super-resolution \u662f\u63d0\u9ad8\u56fe\u50cf, \u89c6\u9891\u5206\u8fa8\u7387\u7684\u4e00\u79cd\u65b9\u5f0f, \u5e7f\u6cdb\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6216\u89c6\u9891\u7f16\u8f91.\n\u5bf9\u4e8e\u672c\u6559\u7a0b, \u6211\u4eec\u5c06\u9996\u5148\u4f7f\u7528\u5e26\u6709\u865a\u62df\u8f93\u5165\u7684\u5c0f\u578b super-resolution \u6a21\u578b.\n\n\u9996\u5148, \u8ba9\u6211\u4eec\u5728 PyTorch \u4e2d\u521b\u5efa\u4e00\u4e2a SuperResolution \u6a21\u578b.\n`\u8fd9\u4e2a\u6a21\u578b <https://github.com/pytorch/examples/blob/master/super_resolution/model.py>`__ \u76f4\u63a5\u6765\u81ea PyTorch \u7684\u4f8b\u5b50\u800c\u6ca1\u6709\u4fee\u6539:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Super Resolution model definition in PyTorch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\nclass SuperResolutionNet(nn.Module):\n    def __init__(self, upscale_factor, inplace=False):\n        super(SuperResolutionNet, self).__init__()\n\n        self.relu = nn.ReLU(inplace=inplace)\n        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.pixel_shuffle(self.conv4(x))\n        return x\n\n    def _initialize_weights(self):\n        init.orthogonal(self.conv1.weight, init.calculate_gain('relu'))\n        init.orthogonal(self.conv2.weight, init.calculate_gain('relu'))\n        init.orthogonal(self.conv3.weight, init.calculate_gain('relu'))\n        init.orthogonal(self.conv4.weight)\n\n# Create the super-resolution model by using the above model definition.\ntorch_model = SuperResolutionNet(upscale_factor=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u901a\u5e38, \u4f60\u73b0\u5728\u8981\u8bad\u7ec3\u8fd9\u4e2a\u6a21\u578b; \u4f46\u662f, \u5bf9\u4e8e\u672c\u6559\u7a0b, \u6211\u4eec\u5c06\u4e0b\u8f7d\u4e00\u4e9b\u9884\u5148\u8bad\u7ec3\u7684\u6743\u91cd.\n\u8bf7\u6ce8\u610f, \u8be5\u6a21\u578b\u6ca1\u6709\u5f97\u5230\u5145\u5206\u8bad\u7ec3\u4ee5\u83b7\u5f97\u826f\u597d\u7684\u51c6\u786e\u6027, \u56e0\u6b64\u4ec5\u7528\u4e8e\u6f14\u793a\u76ee\u7684.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load pretrained model weights\nmodel_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\nbatch_size = 1    # just a random number\n\n# Initialize model with the pretrained weights\nmap_location = lambda storage, loc: storage\nif torch.cuda.is_available():\n    map_location = None\ntorch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n\n# set the train mode to false since we will only run the forward pass.\ntorch_model.train(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5728PyTorch\u4e2d\u5bfc\u51fa\u6a21\u578b\u901a\u8fc7\u8ddf\u8e2a\u5de5\u4f5c.\u8981\u5bfc\u51fa\u6a21\u578b, \u8bf7\u8c03\u7528\u8be5 ``torch.onnx._export()`` \u51fd\u6570.\n\u8fd9\u5c06\u6267\u884c\u6a21\u578b, \u8bb0\u5f55\u8fd0\u7b97\u7b26\u7528\u4e8e\u8ba1\u7b97\u8f93\u51fa\u7684\u8f68\u8ff9.\u7531\u4e8e ``_export`` \u8fd0\u884c\u6a21\u578b, \u6211\u4eec\u9700\u8981\u63d0\u4f9b\u4e00\u4e2a\u8f93\u5165\u5f20\u91cf ``x``.\n\u8fd9\u4e2a\u5f20\u91cf\u4e2d\u7684\u503c\u5e76\u4e0d\u91cd\u8981; \u53ea\u8981\u5c3a\u5bf8\u5408\u9002, \u5b83\u53ef\u4ee5\u662f\u56fe\u50cf\u6216\u968f\u673a\u5f20\u91cf.\n\n\u8981\u4e86\u89e3\u66f4\u591a\u5173\u4e8e PyTorch \u5bfc\u51fa\u754c\u9762\u7684\u7ec6\u8282, \u8bf7\u67e5\u770b `torch.onnx\u6587\u6863 <http://pytorch.org/docs/master/onnx.html>`__.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Input to the model\nx = Variable(torch.randn(batch_size, 1, 224, 224), requires_grad=True)\n\n# Export the model\ntorch_out = torch.onnx._export(torch_model,             # model being run\n                               x,                       # model input (or a tuple for multiple inputs)\n                               \"super_resolution.onnx\", # where to save the model (can be a file or file-like object)\n                               export_params=True)      # store the trained parameter weights inside the model file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``torch_out`` \u662f\u6267\u884c\u6a21\u578b\u540e\u7684\u8f93\u51fa.\u901a\u5e38\u60c5\u51b5\u4e0b, \u60a8\u53ef\u4ee5\u5ffd\u7565\u6b64\u8f93\u51fa,\n\u4f46\u5728\u6b64\u6211\u4eec\u5c06\u4f7f\u7528\u5b83\u6765\u9a8c\u8bc1\u6211\u4eec\u5bfc\u51fa\u7684\u6a21\u578b\u5728 Caffe2 \u4e2d\u8fd0\u884c\u65f6\u8ba1\u7b97\u76f8\u540c\u7684\u503c.\n\n\u73b0\u5728\u6211\u4eec\u6765\u770b\u770b ONNX \u8868\u793a\u6cd5, \u5e76\u5728 Caffe2 \u4e2d\u4f7f\u7528\u5b83.\n\u8fd9\u90e8\u5206\u901a\u5e38\u53ef\u4ee5\u5728\u5355\u72ec\u7684\u8fdb\u7a0b\u6216\u53e6\u4e00\u53f0\u673a\u5668\u4e0a\u5b8c\u6210, \u4f46\u6211\u4eec\u5c06\u7ee7\u7eed\u4f7f\u7528\u76f8\u540c\u7684\u8fc7\u7a0b,\n\u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u9a8c\u8bc1 Caffe2 \u548c PyTorch \u662f\u5426\u4e3a\u7f51\u7edc\u8ba1\u7b97\u76f8\u540c\u7684\u503c:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\nimport onnx_caffe2.backend\n\n# Load the ONNX ModelProto object. model is a standard Python protobuf object\nmodel = onnx.load(\"super_resolution.onnx\")\n\n# prepare the caffe2 backend for executing the model this converts the ONNX model into a\n# Caffe2 NetDef that can execute it. Other ONNX backends, like one for CNTK will be\n# availiable soon.\nprepared_backend = onnx_caffe2.backend.prepare(model)\n\n# run the model in Caffe2\n\n# Construct a map from input names to Tensor data.\n# The graph of the model itself contains inputs for all weight parameters, after the input image.\n# Since the weights are already embedded, we just need to pass the input image.\n# Set the first input.\nW = {model.graph.input[0].name: x.data.numpy()}\n\n# Run the Caffe2 net:\nc2_out = prepared_backend.run(W)[0]\n\n# Verify the numerical correctness upto 3 decimal places\nnp.testing.assert_almost_equal(torch_out.data.cpu().numpy(), c2_out, decimal=3)\n\nprint(\"Exported model has been executed on Caffe2 backend, and the result looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6211\u4eec\u5e94\u8be5\u770b\u5230 PyTorch \u548c Caffe2 \u7684\u8f93\u51fa\u5728\u6570\u5b57\u4e0a\u5339\u914d\u8fbe\u52303\u4f4d\u5c0f\u6570.\n\u4f5c\u4e3a\u65c1\u6ce8, \u5982\u679c\u5b83\u4eec\u4e0d\u5339\u914d, \u90a3\u4e48 Caffe2 \u548c PyTorch \u4e2d\u7684\u64cd\u4f5c\u7b26\u7684\u5b9e\u73b0\u65b9\u5f0f\u4f1a\u6709\u6240\u4e0d\u540c, \u8bf7\u5728\u6b64\u60c5\u51b5\u4e0b\u4e0e\u6211\u4eec\u8054\u7cfb.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u4f7f\u7528 ONNX \u8fc1\u79fb\u5230 SRResNet\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u4f7f\u7528\u4e0e\u4e0a\u8ff0\u76f8\u540c\u7684\u8fc7\u7a0b, \u6211\u4eec\u8fd8\u4e3a `\u672c\u6587 <https://arxiv.org/pdf/1609.04802.pdf>`__  \u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u65b0\u7684 super-resolution \u6a21\u5f0f \"SRResNet\"\n(\u611f\u8c22 Twitter \u4e0a\u7684\u4f5c\u8005\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u53c2\u6570, \u4ee5\u7528\u4e8e\u672c\u6559\u7a0b).\n\u6a21\u578b\u5b9a\u4e49\u548c\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u53ef\u4ee5\u5728 `\u8fd9\u91cc <https://gist.github.com/prigoyal/b245776903efbac00ee89699e001c9bd>`__ \u627e\u5230.\n\u4ee5\u4e0b\u662f SRResNet \u6a21\u578b\u8f93\u5165, \u8f93\u51fa\u7684\u6837\u5b50. |SRResNet|\n\n.. |SRResNet| image:: /_static/img/SRResNet.png\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u6a21\u578b\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u5230\u76ee\u524d\u4e3a\u6b62, \u6211\u4eec\u5df2\u7ecf\u4ece PyTorch \u4e2d\u5bfc\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b, \u5e76\u5c55\u793a\u4e86\u5982\u4f55\u52a0\u8f7d\u5b83\u5e76\u5728 Caffe2 \u4e2d\u8fd0\u884c\u5b83.\n\u73b0\u5728\u8be5\u6a21\u578b\u5df2\u7ecf\u52a0\u8f7d\u5230 Caffe2 \u4e2d, \u6211\u4eec\u53ef\u4ee5\u5c06\u5176\u8f6c\u6362\u4e3a\u9002\u5408 `\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c <https://caffe2.ai/docs/mobile-integration.html>`__ \u7684\u683c\u5f0f.\n\n\u6211\u4eec\u5c06\u4f7f\u7528 Caffe2 \u7684 `mobile\\_exporter <https://github.com/caffe2/caffe2/blob/master/caffe2/python/predictor/mobile_exporter.py>`__ \n\u6765\u751f\u6210\u53ef\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u4e24\u4e2a\u6a21\u578b protobufs.\n\u7b2c\u4e00\u4e2a\u7528\u4e8e\u4f7f\u7528\u6b63\u786e\u7684\u6743\u91cd\u521d\u59cb\u5316\u7f51\u7edc, \u7b2c\u4e8c\u4e2a\u5b9e\u9645\u8fd0\u884c\u7528\u4e8e\u6267\u884c\u6a21\u578b.\n\u6211\u4eec\u5c06\u7ee7\u7eed\u5728\u672c\u6559\u7a0b\u7684\u5176\u4f59\u90e8\u5206\u4f7f\u7528\u5c0f\u578b super-resolution \u6a21\u578b.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# extract the workspace and the model proto from the internal representation\nc2_workspace = prepared_backend.workspace\nc2_model = prepared_backend.predict_net\n\n# Now import the caffe2 mobile exporter\nfrom caffe2.python.predictor import mobile_exporter\n\n# call the Export to get the predict_net, init_net. These nets are needed for running things on mobile\ninit_net, predict_net = mobile_exporter.Export(c2_workspace, c2_model, c2_model.external_input)\n\n# Let's also save the init_net and predict_net to a file that we will later use for running them on mobile\nwith open('init_net.pb', \"wb\") as fopen:\n    fopen.write(init_net.SerializeToString())\nwith open('predict_net.pb', \"wb\") as fopen:\n    fopen.write(predict_net.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``init_net`` \u5c06\u6a21\u578b\u53c2\u6570\u548c\u6a21\u578b\u8f93\u5165\u5d4c\u5165\u5176\u4e2d, ``predict_net`` \u5e76\u5c06\u7528\u4e8e ``init_net`` \u5728\u8fd0\u884c\u65f6\u6307\u5bfc\u6267\u884c.\n\u5728\u672c\u6587\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528 ``init_net`` \u4e0e ``predict_net`` \u4e0a\u9762\u751f\u6210\u548c\u5728\u6b63\u5e38 Caffe2 \u540e\u7aef\u548c\u79fb\u52a8\u8fd0\u884c\u5b83\u4eec, \u5e76\u9a8c\u8bc1\u5728\u4e24\u4e2a\u8bd5\u9a8c\u4e2d\u4ea7\u751f\u7684\u8f93\u51fa\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u732b\u662f\u76f8\u540c\u7684.\n\n\u5728\u672c\u6559\u7a0b\u4e2d, \u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u8457\u540d\u7684\u732b\u54aa\u56fe\u50cf, \u5982\u4e0b\u6240\u793a:\n\n.. figure:: /_static/img/cat_224x224.jpg\n   :alt: cat\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some standard imports\nfrom caffe2.proto import caffe2_pb2\nfrom caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils\n\nimport numpy as np\nimport os\nimport subprocess\nfrom PIL import Image\nfrom matplotlib import pyplot\nfrom skimage import io, transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u9996\u5148, \u6211\u4eec\u52a0\u8f7d\u56fe\u50cf, \u4f7f\u7528\u6807\u51c6\u7684 skimage python \u5e93\u5bf9\u5176\u8fdb\u884c\u9884\u5904\u7406.\n\u8bf7\u6ce8\u610f, \u8fd9\u79cd\u9884\u5904\u7406\u662f training/testing \u795e\u7ecf\u7f51\u7edc\u5904\u7406\u6570\u636e\u7684\u6807\u51c6\u5b9e\u8df5.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# load the image\nimg_in = io.imread(\"./_static/img/cat.jpg\")\n\n# resize the image to dimensions 224x224\nimg = transform.resize(img_in, [224, 224])\n\n# save this resized image to be used as input to the model\nio.imsave(\"./_static/img/cat_224x224.jpg\", img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u73b0\u5728, \u4f5c\u4e3a\u4e0b\u4e00\u6b65, \u6211\u4eec\u6765\u8c03\u6574\u5927\u5c0f\u7684\u732b\u56fe\u50cf, \u5e76\u5728 Caffe2 \u540e\u7aef\u8fd0\u884c super-resolution \u6a21\u578b\u5e76\u4fdd\u5b58\u8f93\u51fa\u56fe\u50cf.\n\u56fe\u50cf\u5904\u7406\u6b65\u9aa4\u5982\u4e0b\u5df2\u4ece PyTorch \u5b9e\u73b0 super-resolution \u6a21\u578b\u91c7\u7528 `\u8fd9\u91cc <https://github.com/pytorch/examples/blob/master/super_resolution/super_resolve.py>`__\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# load the resized image and convert it to Ybr format\nimg = Image.open(\"./_static/img/cat_224x224.jpg\")\nimg_ycbcr = img.convert('YCbCr')\nimg_y, img_cb, img_cr = img_ycbcr.split()\n\n# Let's run the mobile nets that we generated above so that caffe2 workspace is properly initialized\nworkspace.RunNetOnce(init_net)\nworkspace.RunNetOnce(predict_net)\n\n# Caffe2 has a nice net_printer to be able to inspect what the net looks like and identify\n# what our input and output blob names are.\nprint(net_printer.to_string(predict_net))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u4ece\u4e0a\u9762\u7684\u8f93\u51fa\u4e2d, \u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8f93\u5165\u540d\u4e3a \"9\", \u8f93\u51fa\u540d\u4e3a \"27\"(\u6709\u70b9\u5947\u602a, \u6211\u4eec\u5c06\u6570\u5b57\u4f5c\u4e3a blob \u540d\u79f0, \u4f46\u8fd9\u662f\u56e0\u4e3a\u8ddf\u8e2a JIT \u4f1a\u4e3a\u6a21\u578b\u751f\u6210\u7f16\u53f7\u6761\u76ee)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Now, let's also pass in the resized cat image for processing by the model.\nworkspace.FeedBlob(\"9\", np.array(img_y)[np.newaxis, np.newaxis, :, :].astype(np.float32))\n\n# run the predict_net to get the model output\nworkspace.RunNetOnce(predict_net)\n\n# Now let's get the model output blob\nimg_out = workspace.FetchBlob(\"27\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u73b0\u5728, \u6211\u4eec\u5c06\u8fd4\u56de\u53c2\u8003 PyTorch \u6267\u884c super-resolution \u6a21\u578b\u7684\u540e\u5904\u7406\u6b65\u9aa4,\n`\u5728\u8fd9\u91cc <https://github.com/pytorch/examples/blob/master/super_resolution/super_resolve.py>`__ \u6784\u5efa\u56de\u6700\u7ec8\u8f93\u51fa\u7684\u56fe\u50cf\u5e76\u4fdd\u5b58\u56fe\u50cf.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "img_out_y = Image.fromarray(np.uint8((img_out[0, 0]).clip(0, 255)), mode='L')\n\n# get the output image follow post-processing step from PyTorch implementation\nfinal_img = Image.merge(\n    \"YCbCr\", [\n        img_out_y,\n        img_cb.resize(img_out_y.size, Image.BICUBIC),\n        img_cr.resize(img_out_y.size, Image.BICUBIC),\n    ]).convert(\"RGB\")\n\n# Save the image, we will compare this with the output image from mobile device\nfinal_img.save(\"./_static/img/cat_superres.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u6211\u4eec\u5df2\u7ecf\u5b8c\u6210\u4e86\u5728\u7eaf Caffe2 \u540e\u7aef\u8fd0\u884c\u6211\u4eec\u7684\u79fb\u52a8\u7f51\u7edc, \u73b0\u5728, \u8ba9\u6211\u4eec\u5728 Android \u8bbe\u5907\u4e0a\u6267\u884c\u6a21\u578b\u5e76\u83b7\u53d6\u6a21\u578b\u8f93\u51fa.\n\n``NOTE``: \u5bf9\u4e8e Android \u5f00\u53d1, ``adb`` \u9700\u8981\u4f7f\u7528 shell, \u5426\u5219\u4ee5\u4e0b\u90e8\u5206\u6559\u7a0b\u5c06\u65e0\u6cd5\u8fd0\u884c.\n\n\u5728\u6211\u4eec\u7684\u79fb\u52a8\u8bbe\u5907 runnig \u6a21\u578b\u7684\u7b2c\u4e00\u6b65\u4e2d, \u6211\u4eec\u5c06\u628a\u79fb\u52a8\u8bbe\u5907\u7684\u672c\u5730\u901f\u5ea6\u57fa\u51c6\u4e8c\u8fdb\u5236\u6587\u4ef6\u63a8\u9001\u5230 adb.\n\u8fd9\u4e2a\u4e8c\u8fdb\u5236\u6587\u4ef6\u53ef\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u6a21\u578b, \u4e5f\u53ef\u4ee5\u5bfc\u51fa\u7a0d\u540e\u53ef\u4ee5\u68c0\u7d22\u7684\u6a21\u578b\u8f93\u51fa.\n\u4e8c\u8fdb\u5236\u6587\u4ef6 `\u5728\u8fd9\u91cc <https://github.com/caffe2/caffe2/blob/master/caffe2/binaries/speed_benchmark.cc>`__ \u53ef\u7528.\n\u4e3a\u4e86\u6784\u5efa\u4e8c\u8fdb\u5236\u6587\u4ef6, \u8bf7 ``build_android.sh`` \u6309\u7167 `\u6b64\u5904 <https://github.com/caffe2/caffe2/blob/master/scripts/build_android.sh>`__ \u7684\u8bf4\u660e\u6267\u884c\u811a\u672c.\n\n``NOTE``: \u60a8\u9700\u8981 ``ANDROID_NDK`` \u5b89\u88c5\u5e76\u8bbe\u7f6e\u60a8\u7684 env \u53d8\u91cf ``ANDROID_NDK=path to ndk root``\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# let's first push a bunch of stuff to adb, specify the path for the binary\nCAFFE2_MOBILE_BINARY = ('caffe2/binaries/speed_benchmark')\n\n# we had saved our init_net and proto_net in steps above, we use them now.\n# Push the binary and the model protos\nos.system('adb push ' + CAFFE2_MOBILE_BINARY + ' /data/local/tmp/')\nos.system('adb push init_net.pb /data/local/tmp')\nos.system('adb push predict_net.pb /data/local/tmp')\n\n# Let's serialize the input image blob to a blob proto and then send it to mobile for execution.\nwith open(\"input.blobproto\", \"wb\") as fid:\n    fid.write(workspace.SerializeBlob(\"9\"))\n\n# push the input image blob to adb\nos.system('adb push input.blobproto /data/local/tmp/')\n\n# Now we run the net on mobile, look at the speed_benchmark --help for what various options mean\nos.system(\n    'adb shell /data/local/tmp/speed_benchmark '                     # binary to execute\n    '--init_net=/data/local/tmp/super_resolution_mobile_init.pb '    # mobile init_net\n    '--net=/data/local/tmp/super_resolution_mobile_predict.pb '      # mobile predict_net\n    '--input=9 '                                                     # name of our input image blob\n    '--input_file=/data/local/tmp/input.blobproto '                  # serialized input image\n    '--output_folder=/data/local/tmp '                               # destination folder for saving mobile output\n    '--output=27,9 '                                                 # output blobs we are interested in\n    '--iter=1 '                                                      # number of net iterations to execute\n    '--caffe2_log_level=0 '\n)\n\n# get the model output from adb and save to a file\nos.system('adb pull /data/local/tmp/27 ./output.blobproto')\n\n\n# We can recover the output content and post-process the model using same steps as we followed earlier\nblob_proto = caffe2_pb2.BlobProto()\nblob_proto.ParseFromString(open('./output.blobproto').read())\nimg_out = utils.Caffe2TensorToNumpyArray(blob_proto.tensor)\nimg_out_y = Image.fromarray(np.uint8((img_out[0,0]).clip(0, 255)), mode='L')\nfinal_img = Image.merge(\n    \"YCbCr\", [\n        img_out_y,\n        img_cb.resize(img_out_y.size, Image.BICUBIC),\n        img_cr.resize(img_out_y.size, Image.BICUBIC),\n    ]).convert(\"RGB\")\nfinal_img.save(\"./_static/img/cat_superres_mobile.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u73b0\u5728, \u60a8\u53ef\u4ee5\u6bd4\u8f83\u56fe\u50cf ``cat_superres.jpg`` (\u6765\u81ea\u7eaf caffe2 \u540e\u7aef\u6267\u884c\u7684 ``cat_superres_mobile.jpg`` \u6a21\u578b\u8f93\u51fa) \u548c (\u6765\u81ea\u79fb\u52a8\u6267\u884c\u7684\u6a21\u578b\u8f93\u51fa) \u5e76\u67e5\u770b\u8fd9\u4e24\u4e2a\u56fe\u50cf\u770b\u8d77\u6765\u76f8\u540c.\n\u5982\u679c\u5b83\u4eec\u770b\u8d77\u6765\u4e0d\u4e00\u6837, \u90a3\u4e48\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u5c31\u4f1a\u51fa\u73b0\u95ee\u9898, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u8bf7\u8054\u7cfb Caffe2 \u793e\u533a.\n\u60a8\u5e94\u8be5\u671f\u671b\u770b\u5230\u8f93\u51fa\u56fe\u50cf\u5982\u4e0b\u6240\u793a:\n\n.. figure:: /_static/img/cat_output1.png\n   :alt: output\\_cat\n\n\n\u4f7f\u7528\u4e0a\u8ff0\u6b65\u9aa4, \u60a8\u53ef\u4ee5\u8f7b\u677e\u5730\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u6a21\u578b.\n\u53e6\u5916, \u6709\u5173 caffe2 \u79fb\u52a8\u540e\u7aef\u7684\u66f4\u591a\u4fe1\u606f, \u8bf7\u67e5\u770b `caffe2-android-demo <https://caffe2.ai/docs/AI-Camera-demo-android.html>`__.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}